{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m : number of training examples\n",
    "m = 5\n",
    "input_features = 2\n",
    "output_layer = 1\n",
    "\n",
    "# X = (input_features, training_examples) = training_data_shape\n",
    "X = np.random.rand(input_features , m)\n",
    "\n",
    "# Y = output labels (or classes)\n",
    "Y = np.random.rand(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First layer calcualtions (Hidden layer)\n",
    "\n",
    "**Here is model from deeplearnig.ai course**:\n",
    "<img src=\"classification_kiank.png\" style=\"width:600px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "   * w1, w2, w3, w4 are weights for 4 neurons in hidden layer.\n",
    "   * b1, b2, b3, b4 are biases for hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer outputs for all training examples : (4, 5)\n",
      "Final layer outputs for all training examples (i.e. predictions) : (5,)\n",
      "[0.93407018 0.94632001 0.95783107 0.9573956  0.96062289]\n"
     ]
    }
   ],
   "source": [
    "################ First Layer Calculations ################\n",
    "np.random.seed(5)\n",
    "w1 = np.random.rand(1,2)\n",
    "w2 = np.random.rand(1,2)\n",
    "w3 = np.random.rand(1,2)\n",
    "w4 = np.random.rand(1,2)\n",
    "\n",
    "b1 = np.random.rand(1,1)\n",
    "b2 = np.random.rand(1,1)\n",
    "b3 = np.random.rand(1,1)\n",
    "b4 = np.random.rand(1,1)\n",
    "\n",
    "z1 = np.dot(w1, X) + b1\n",
    "z2 = np.dot(w2, X) + b2\n",
    "z3 = np.dot(w3, X) + b3\n",
    "z4 = np.dot(w4, X) + b4\n",
    "\n",
    "# Final activations (calculated individually)\n",
    "a1 = sigmoid(z1)\n",
    "a2 = sigmoid(z2)\n",
    "a3 = sigmoid(z3)\n",
    "a4 = sigmoid(z4)\n",
    "\n",
    "# Here stacked all activations in hidden layer just for comparing with full vectorized version later\n",
    "A1 = np.stack((a1,a2,a3,a4), axis=0)\n",
    "A1 = np.squeeze(A1).reshape((A1.shape[0], m))\n",
    "print(\"Hidden layer outputs for all training examples :\", A1.shape)\n",
    "\n",
    "################ Second Layer Calculations ################\n",
    "np.random.seed(4)\n",
    "W2 = np.random.rand(4)\n",
    "B2 = np.random.rand(1)\n",
    "Z2 = np.dot(W2, A1) + B2\n",
    "A2 = sigmoid(Z2)\n",
    "\n",
    "print(\"Final layer outputs for all training examples (i.e. predictions) :\" , A2.shape)\n",
    "print(A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using stacking as explained in lecture PPT\n",
    "    * Here used only single matrix W1 and B1. (which is stacked version of w1, w2, w3, w4 for previous explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer outputs for all training examples : (4, 5)\n",
      "Final layer outputs for all training examples (i.e. predictions) : (5,)\n",
      "[0.93407018 0.94632001 0.95783107 0.9573956  0.96062289]\n"
     ]
    }
   ],
   "source": [
    "################ First Layer Calculations ################\n",
    "np.random.seed(5)\n",
    "W1 = np.random.rand(4, 2)\n",
    "B1 = np.random.rand(4, 1)\n",
    "Z1 = np.dot(W1, X) + B1\n",
    "A1 = sigmoid(Z1)\n",
    "\n",
    "print(\"Hidden layer outputs for all training examples :\", A1.shape)\n",
    "\n",
    "################ Second Layer Calculations ################\n",
    "np.random.seed(4)\n",
    "W2 = np.random.rand(4)\n",
    "B2 = np.random.rand(1)\n",
    "Z2 = np.dot(W2, A1) + B2\n",
    "A2 = sigmoid(Z2)\n",
    "print(\"Final layer outputs for all training examples (i.e. predictions) :\" , A2.shape)\n",
    "print(A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now with activation vector you can calculate class (Binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Below code is read as : (if value in A2 is greater than 0.5, make it 1, else make it 0)\n",
    "# No loop version\n",
    "predictions = np.where(A2 > 0.5, 1, 0)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
